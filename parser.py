from db import ad_exists, add_ad
import requests
import random
import re
from bs4 import BeautifulSoup

def clean_text(text: str) -> str:
    return re.sub(r"\s+", " ", text.strip())

async def get_random_cars(
    min_price=500,
    max_price=3000,
    count=1,
    max_photos=10,
    max_pages=20,
    base_url: str | None = None
):
    headers = {"User-Agent": "Mozilla/5.0"}

    if base_url:
        urls = [base_url]
    else:
        urls = [
            f"https://cars.av.by/filter?price_usd[min]={min_price}&price_usd[max]={max_price}&page={random.randint(1, 10)}"
            for _ in range(max_pages)
        ]

    for url in urls:
        try:
            response = requests.get(url, headers=headers, timeout=10)
        except requests.RequestException:
            continue

        soup = BeautifulSoup(response.text, "html.parser")
        items = soup.find_all("div", class_="listing-item")
        if not items:
            continue

        random.shuffle(items)
        results = []

        for random_item in items:
            if len(results) >= count:
                break

            title_tag = random_item.find("a", class_="listing-item__link")
            link = "https://cars.av.by" + title_tag["href"] if title_tag else ""
            if not link or await ad_exists(link):
                continue

            title = title_tag.text.strip() if title_tag else "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"

            price_tag = random_item.find("div", class_="listing-item__price-secondary")
            price_text = price_tag.text.strip().replace("‚âà", "").replace(" ", "") if price_tag else "‚Äî"

            location_tag = random_item.find("div", class_="listing-item__location")
            location_text = location_tag.text.strip() if location_tag else "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"

            # --- –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ card__params ---
            params_block = random_item.find("div", class_="listing-item__params")
            params_text = params_block.get_text(", ", strip=True) if params_block else ""
            params_text = clean_text(params_text)
            parts = [p.strip() for p in params_text.split(",") if p.strip()]

            year = next((p for p in parts if re.match(r"\d{4}", p)), "‚Äî").replace("–≥.", "").strip()
            mileage = next((p for p in parts if "–∫–º" in p), "‚Äî")
            transmission = next((p for p in parts if any(t in p.lower() for t in ["–º–µ—Ö–∞–Ω–∏–∫–∞", "–∞–≤—Ç–æ–º–∞—Ç", "–≤–∞—Ä–∏–∞—Ç–æ—Ä", "—Ä–æ–±–æ—Ç"])), "‚Äî")

            # —Ç–∏–ø –¥–≤–∏–≥–∞—Ç–µ–ª—è –∏ –æ–±—ä—ë–º
            engine_info = "‚Äî"
            engine_type = next((p for p in parts if any(x in p.lower() for x in ["–±–µ–Ω–∑–∏–Ω", "–¥–∏–∑–µ–ª—å", "—ç–ª–µ–∫—Ç—Ä–æ", "–≥–∏–±—Ä–∏–¥", "–≥–∞–∑"])), "")
            engine_volume = next((p for p in parts if "–ª" in p and not "–∫–º" in p), "")
            if engine_type or engine_volume:
                engine_info = f"{engine_type}, {engine_volume}".strip(", ")

            # --- –≤—ã—Ç–∞—Å–∫–∏–≤–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –∏–∑ –æ–±—ä—è–≤–ª–µ–Ω–∏—è ---
            adv_soup = None
            try:
                adv_resp = requests.get(link, headers=headers, timeout=10)
                adv_soup = BeautifulSoup(adv_resp.text, "html.parser")
            except requests.RequestException:
                pass

            description = "–ù–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è"
            if adv_soup:
                comment_block = adv_soup.find("div", class_="card__comment")
                if comment_block:
                    # –ë–µ—Ä—ë–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç –±–ª–æ–∫–∞, –∞ –Ω–µ —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–π <p>
                    description = clean_text(comment_block.get_text(" ", strip=True))

            # --- –ø—Ä–∏–≤–æ–¥ –∏–∑ card__description ---
            drive = "‚Äî"
            if adv_soup:
                desc_block = adv_soup.find("div", class_="card__description")
                if desc_block:
                    desc_text = clean_text(desc_block.get_text(" ", strip=True))
                    drive = next((p for p in desc_text.split(",") if "–ø—Ä–∏–≤–æ–¥" in p.lower()), "‚Äî").strip()

            # --- —Ñ–æ—Ç–æ ---
            photos = []
            if adv_soup:
                gallery = adv_soup.select(".gallery__stage .gallery__frame img")
                for img in gallery:
                    url_img = img.get("data-src") or img.get("src")
                    if url_img and not url_img.startswith("data:image"):
                        photos.append(url_img)
                    if len(photos) >= max_photos:
                        break

            formatted_message = (
                f"üöó {title}  üìÖ {year}\n"
                f"üõ£ {mileage}  |‚õΩÔ∏è {engine_info}\n"
                f"üì¶ {transmission.title()} |‚öôÔ∏è {drive}\n"
                f"üìç {location_text}\n"
                f"üí∞ {price_text}\n\n"
                f"{description}\n\n"
            )

            results.append({
                "title": title,
                "price": price_text,
                "location": location_text,
                "params": params_text,
                "description": description,
                "link": link,
                "photos": photos,
                "drive": drive,
                "engine_info": engine_info,
                "message": formatted_message
            })

            await add_ad(link)

        if results:
            return results

    return []


def clean_text(text: str) -> str:
    """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏ –∑–∞–º–µ–Ω—è–µ—Ç –Ω–µ—Ä–∞–∑—Ä—ã–≤–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã, –ø—Ä–æ–±–µ–ª—ã –∏ –∑–∞–ø—è—Ç—ã–µ –≤ —á–∏—Å–ª–∞—Ö."""
    if not text:
        return ""
    # –∑–∞–º–µ–Ω—è–µ–º –Ω–µ—Ä–∞–∑—Ä—ã–≤–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –∏ —É–∑–∫–∏–µ –ø—Ä–æ–±–µ–ª—ã
    text = text.replace("\xa0", " ").replace("‚Äâ", " ")
    # –∑–∞–º–µ–Ω—è–µ–º –∑–∞–ø—è—Ç—ã–µ –≤ —á–∏—Å–ª–∞—Ö –Ω–∞ —Ç–æ—á–∫–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 1,8 ‚Üí 1.8)
    text = re.sub(r"(\d),(\d)", r"\1.\2", text)
    # —É–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    text = re.sub(r"\s+", " ", text)
    # —É–±–∏—Ä–∞–µ–º –¥–≤–æ–π–Ω—ã–µ –∑–∞–ø—è—Ç—ã–µ
    text = re.sub(r"(,\s*){2,}", ", ", text)
    return text.strip(",. \n\t")

def parse_single_car(url, max_photos=10):
    import re
    import requests
    from bs4 import BeautifulSoup

    headers = {"User-Agent": "Mozilla/5.0"}
    response = requests.get(url, headers=headers, timeout=10)
    if response.status_code != 200:
        return None

    soup = BeautifulSoup(response.text, "html.parser")

    # üè∑ –ó–∞–≥–æ–ª–æ–≤–æ–∫
    title_block = soup.find("h1")
    title = clean_text2(title_block.text) if title_block else "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"
    title = re.sub(r"(?i)^–ü—Ä–æ–¥–∞–∂–∞\s+", "", title).strip()
    title = re.sub(r",?\s*\d{4}\s*–≥.*$", "", title)
    title = re.sub(r"\s+–≤\s+[A-–Ø–ÅA-Z][–∞-—è—ëa-z]+.*$", "", title)
    title = title.strip(",. ")

    # üß© –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–≥–æ–¥, —Ç—Ä–∞–Ω—Å–º–∏—Å—Å–∏—è, –æ–±—ä—ë–º, —Ç–æ–ø–ª–∏–≤–æ, –ø—Ä–æ–±–µ–≥)
    params_block = soup.find("div", class_="card__params")
    params_text = clean_text2(params_block.get_text(" ", strip=True)) if params_block else ""

    year = gearbox = engine = fuel = mileage = "‚Äî"

    # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–±—ä—ë–º –¥–≤–∏–≥–∞—Ç–µ–ª—è –¥–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è
    m = re.search(r"(\d+[.,]?\d*)\s*–ª", params_text.lower())
    if m:
        engine = m.group(1).replace(",", ".") + " –ª"

    # –¢–µ–ø–µ—Ä—å —Ä–∞–∑–±–∏–≤–∞–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ –∑–∞–ø—è—Ç—ã–º
    parts = [p.strip() for p in params_text.split(",") if p.strip()]
    for p in parts:
        p_low = p.lower()

        if re.search(r"\d{4}\s*–≥", p_low):
            year = p.replace("–≥.", "").replace("–≥", "").strip()

        elif any(x in p_low for x in ["–º–µ—Ö–∞–Ω–∏–∫–∞", "–∞–≤—Ç–æ–º–∞—Ç", "–≤–∞—Ä–∏–∞—Ç–æ—Ä", "—Ä–æ–±–æ—Ç"]):
            gearbox = p

        elif any(x in p_low for x in ["–±–µ–Ω–∑–∏–Ω", "–¥–∏–∑–µ–ª—å", "–≥–∞–∑", "—ç–ª–µ–∫—Ç—Ä–æ", "–≥–∏–±—Ä–∏–¥"]):
            fuel = p

        elif "–∫–º" in p_low:
            mileage = p

    # üöó –ö—É–∑–æ–≤, –ø—Ä–∏–≤–æ–¥, —Ü–≤–µ—Ç
    desc_block = soup.find("div", class_="card__description")
    desc_text = clean_text2(desc_block.get_text(", ", strip=True)) if desc_block else "‚Äî"
    drive = next((p.strip() for p in desc_text.split(",") if "–ø—Ä–∏–≤–æ–¥" in p.lower()), "‚Äî")

    # ‚öôÔ∏è –ú–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—è
    mod_block = soup.find("div", class_="card__modification")
    mod_text = clean_text2(mod_block.get_text(" ", strip=True)) if mod_block else "‚Äî"
    if "–í—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã" in mod_text:
        mod_text = mod_text.replace("–í—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã", "").strip(",. ")

    # üìç –õ–æ–∫–∞—Ü–∏—è
    loc_block = soup.find("div", class_="card__location")
    location = clean_text2(loc_block.text) if loc_block else "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"

    # üí∞ –¶–µ–Ω–∞
    price_block = soup.find("div", class_="card__price-secondary")
    price = price_block.text.strip().replace("‚âà", "").replace(" ", "") if price_block else "‚Äî"

    # üìù –û–ø–∏—Å–∞–Ω–∏–µ
    comment_block = soup.find("div", class_="card__comment")
    if comment_block:
        # –¥–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å " " –º–µ–∂–¥—É —Å—Ç—Ä–æ–∫–∞–º–∏
        description = clean_text2(comment_block.get_text(" ", strip=True))
        description = re.sub(r"(?i)^–û–ø–∏—Å–∞–Ω–∏–µ", "", description).strip()
    else:
        description = "–ù–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è"

    MAX_TG_CAPTION = 900
    if len(description) > MAX_TG_CAPTION:
        description = description[:MAX_TG_CAPTION - 3].rsplit(" ", 1)[0] + "..."

    # üñº –§–æ—Ç–æ
    photos = []
    gallery = soup.select(".gallery__stage .gallery__frame img")
    for img in gallery:
        src = img.get("data-src") or img.get("src")
        if src and not src.startswith("data:image"):
            photos.append(src)
        if len(photos) >= max_photos:
            break

    # ‚ùóÔ∏è –£–¥–∞–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Ñ–æ—Ç–æ, –µ—Å–ª–∏ –æ–Ω–æ –¥—É–±–ª–∏—Ä—É–µ—Ç –ø—Ä–µ–¥–ø–æ—Å–ª–µ–¥–Ω–µ–µ
    if len(photos) > 1 and photos[-1] == photos[-2]:
        photos.pop()


    # üõ† –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ç–∏–ø –∏ –æ–±—ä—ë–º –¥–≤–∏–≥–∞—Ç–µ–ª—è
    engine_info = "‚Äî"
    if fuel != "‚Äî" or engine != "‚Äî":
        engine_info = f"{fuel}, {engine}".strip(", ")

    return {
        "title": title,
        "year": year,
        "mileage": mileage,
        "gearbox": gearbox,
        "drive": drive,
        "engine_info": engine_info,
        "location": location,
        "price": price,
        "description": description,
        "photos": photos,
        "link": url,
    }


def clean_text2(text: str) -> str:
    """–£–±–∏—Ä–∞–µ—Ç –º—É—Å–æ—Ä–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã, –Ω–µ—Ä–∞–∑—Ä—ã–≤–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –∏ –¥–≤–æ–π–Ω—ã–µ –∑–∞–ø—è—Ç—ã–µ"""
    import re
    if not text:
        return ""
    text = text.replace("\xa0", " ").replace("‚Äâ", " ")
    text = re.sub(r"\s+", " ", text)
    text = re.sub(r"(,\s*){2,}", ", ", text)
    return text.strip(",. \n\t")




